# 今回のプロジェクトについて
今回は自分が所属している大学の研究室のコミュニケーションツールであるslackから教授の発言を収集して、
そのデータからその教授っぽい返しをする言語生成AIを作ることができるのではないかと思いました。結論から
言うと、まだ完成していません(もしかしたらできないのかも)。そのためここではここまで自分の考え
てきたアイデアと作ってきたコードの説明をしたのち、現在抱えている問題と今後の展望について述べたいと思
います。

## 1: ここまでの成果
ここではここまで自分が踏んできたステップについて説明していきます。大きく分けて以下のようになります
1: slack APIを取得
2: slack APIを使ってテキスト収集
3: 前処理
4: データをエンコーディング(前処理の続き)
4: GPT2を使って転移学習

以下でそれぞれ紹介していきます
### 1.1: slack APIの取得
まずslackの過去のテキストを取得するにはslack APIトークンを取得しなくてはいけません。APIトークンの
取得方法について説明していきます。基本と気にslack APIのサイトからcreate new appで新しいアプリをつ
くることで過去のテキストデータを取得することができます。
![alt text](pic/pict1.png)

進んだのち、左タスクバーのOAuth & Permissionに進みます。ここでscopeを設定するのですがここで以下の
scopeを追加します。

1つ目   app_mentions:read

slackアプリがメンションされたメッセージを読み取ることができます。

2つ目   channels:history

slackアプリが追加されたパブリックチャンネル内のメッセージやその他コンテンツを読み取ることができます。
つまり過去のメッセージを集めることができます。(これが一番大事)

3つ目   channels:join

slackアプリがパブリックチャンネルに参加することが可能になります。パブリックチャンネルのメッセージを
読み取るためにはそのチャンネル参加する必要があるためこのスコープが必要です。

4つ目   users:read

ユーザの情報を取得します。今回は教授の発言を収集するためユーザ情報が必要になると思い追加しました。

基本的に以上の4つを追加してアプリを起動するだけで大丈夫です。するとAPIトークンが取得できこのトークンを
用いて過去のテキストを収集していきます。

### 1.2: APIトークンを使ってテキスト収集
まず以下のライブラリをインストールします。
```python
pip install slack_sdk
```

このライブラリでslackのAPIと連携しながらコーディングすることができます。次にいかのコード(関数)について
説明します。
```python
import os
from slack_sdk import WebClient
#my_slack_tokenが取得したAPIトークンを表します。
slack_token = "my_slack_token"
client = WebClient(token=slack_token)
cursor=None

def collect_txt_data(channel_id):
    client.conversations_join(channel=channel_id)
    response = client.conversations_history(channel=channel_id)
    messages = response["messages"]
    
    while 'response_metadata' in response and 'next_cursor' in response['response_metadata']:
        cursor = response['response_metadata']['next_cursor']
        response = client.conversations_history(channel=channel_id, limit=200, cursor=cursor)
        messages.extend(response['messages'])

    texts = [msg["text"] for msg in messages if "text" in msg and "subtype" not in msg]
    users = [msg["user"] for msg in messages if "text" in msg and "subtype" not in msg]
    res = [users, texts]
    return res
```

まず取得したslackトークンをもとにclientをAPIと連携させます。その後上の関数collect_txt_data()についてですが
これはまずclientをチャンネルに参加させ、そのチャンネル内のメッセージを取得したのち、ページネーションをつかってその
チャンネル内のすべてのメッセージを取得します。そしてmessageデータからtextsにメッセージ内容をusersにそのメッセージ
を書いたuserIDを格納します。短く言うとcollect_txt_data(channel_id)は指定されたchannelの全てのテキストそのテキスト
のそれぞれのuser_idを返す関数です。この関数を用いて以下のコードを実行させると
```python
b4_channel = collect_txt_data(channel_id="C07B6J6U6CR")
gen_channel = collect_txt_data(channel_id="C027L2L0L5S")
rnd_channel = collect_txt_data(channel_id="C027ARC13DK")
book_channel = collect_txt_data(channel_id="C026Z4TNWF9")
dplearn_channel = collect_txt_data(channel_id="C034N2LMRKM")
els_channel = collect_txt_data(channel_id="C02D3MY7057")
```
これでそれぞれのチャンネルから全てのテキストとそのテキストを書いたuser_idを取得することができます。
この次からはこれらのテキストをモデルに渡すためにテキストに前処理を施します。

### 1.3: 前処理
対話型のAIを用いるためにはデータは対話型でなくてはいけません。そこでデータを(入力テキスト, ターゲットテキスト)
という形にする必要があります。つまりここでは(誰かほかの人の発言や質問, 教授の返答)という形です。それを行うのが
以下のmake_input_data関数です。
```python
def make_input_data(channel):
    max_length = 256
    data = []
    for i in range(1, len(channel[0])):
        if channel[0][i-1] == "U026Z4RQXM5" and channel[0][i] != "U026Z4RQXM5" and "<https:" not in channel[1][i-1] and "<https:" not in channel[1][i]:
            data.append((channel[1][i], channel[1][i-1]))

    return data
```
ここでは単純に教授の発言とその一つ前の発言をセットにしてdataに格納しているだけですが(データ数が少なすぎるため)。例えばmessageにはメッセージ時刻等の情報もあり、そこから誰かの発言から1時間以内に教授の発言あったときのみdataに格納するといった処理もできると思われます。また教授がリンクを張って生徒にアナウンスするメッセージが多くこれは対話データに不向きと考えメッセージにurlが含まれているものはデータに含むことはしませんでした。この関数を用いて以下のコードを実行すると。
```python
b4_data = make_input_data(b4_channel)
gen_data = make_input_data(gen_channel)
rnd_data = make_input_data(rnd_channel)
book_data = make_input_data(book_channel)
dplearn_data = make_input_data(dplearn_channel)
els_data = make_input_data(dplearn_channel)

all_data= b4_data+gen_data+rnd_data+book_data+dplearn_data+els_data
```
データが適切な形に変更され、allに全てのチャンネルのデータが入ります。

またこの時集まった対話のペアがわずか100ほど(完全に予想外)で明らかに少ないと考えたため、以下のコードで公開されている対話データを補充しました。

```python

import requests
import json

file_path = r'ここでファイルのパスを指定します'

# JSONファイルを読み込む
with open(file_path, 'r', encoding='utf-8') as file:
    data = json.load(file)

subset = data[:1000]
subdata = []

for k in range(1000):
    for i in range(8):
        subdata.append((subset[k]["qas"][i]["question"], subset[k]["qas"][i]["answer"]))

all = all_data + subdata
```

正直これはもはやslackのデータなど関係なくなってしまいアウトだと思いましたがデータ数100はさすがに少なすぎると思ったため苦渋の決断でした。

### 1.4: エンコーディング
次にこの対話データをエンコーディングします。srcに入力データ、trgにターゲットデータが入っています。以下のコードで対話データをトークン化します。
```python
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments

# トークナイザーとモデルのロード
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained('gpt2')

# トークン化とエンコーディング
max_length = 132

src_encodings = tokenizer(src[100:180], padding='max_length', truncation=True, max_length=max_length, return_tensors="pt")
trg_encodings = tokenizer(trg[100:180], padding='max_length', truncation=True, max_length=max_length, return_tensors="pt")
```
ここで大事なのはsrc_encodingsとtrg_encodingsでサイズを合わせる必要があるため。パディングを行いどちらもサイズを132に合わせます。その後に以下のコードを実行してデータセットを作ります。
```python
import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, src_encodings, trg_encodings):
        self.src_encodings = src_encodings
        self.trg_encodings = trg_encodings

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.src_encodings.items()}
        item['labels'] = self.trg_encodings['input_ids'][idx].clone().detach()
        return item

    def __len__(self):
        return len(self.src_encodings['input_ids'])

dataset = CustomDataset(tmp_src, tmp_trg)
```
ここで何をしているのかというとgpt2のモデルは教師あり学習をしてキー"labels"をターゲットにて学習をします。そこでそれ以外のキーをsrc_encodingsのデータで、キー"labels"にはtrg_encodingのデータを対応させることでモデルに適したデータの型に変換します。

### 1.5: モデルの学習
ここまで出来たら後はモデルにdatasetを学習させて終わりです。
```python
from transformers import GPT2LMHeadModel, Trainer, TrainingArguments

# モデルのロード
model = GPT2LMHeadModel.from_pretrained('gpt2')

# トレーニング設定
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# トレーナーの設定
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

# トレーニングの実行
trainer.train()
```

## 2: 現状の問題とこれからの展望
ですが初めに言った通りまだ思うような結果を得ることは出来ていません。というのもコードでも示した通り、予測したデータが文字化けしてしまうという現象が起きてしまっています。なぜこのような問題が起きてしまっているのか探っている段階ですが以下の問題ではないかと考えています。

学習させているデータ数が少なすぎる

モデルの設定がおかしい

まずより多くのデータを学習させたいと思ってはいるのですが計算にとてつもない時間がかかってしまうため、GPUやTPUを用いてより大規模なデータ学習させても文字化けが起きるのかを検証していきたいと思います。

ただ調べたところgpt2の転移学習ではデータ量が少なかろうと文字化けの現象が起きるとは考えずらいらしくもしかしたら別のところに原因はあるのかもしれません(例えばモデルの設定)。エンコードとデコードには問題はないと思います。(学習データを一度エンコードしてデコードしたところ元のテキストがきちんと表示された)。

成果が出ずとても残念ですがとりあえず途中経過としてこのポートフォリオを提出させていただきます。
